//
//  MLXService.swift
//  RunAnywhereAI
//
//  Created by Sanchit Monga on 7/26/25.
//

import Foundation

// MLX Framework imports - now available since MLX-Swift 0.25.6 is installed
#if canImport(MLX)
import MLX
#endif
#if canImport(MLXNN)
import MLXNN
#endif
#if canImport(MLXRandom)
import MLXRandom
#endif
#if canImport(MLXLMCommon)
import MLXLMCommon
#endif

// MARK: - MLX Model Wrapper

private class MLXModelWrapper {
    var modelPath: String
    var model: Any? // Will hold the actual MLX model
    var tokenizer: Any? // Will hold the actual MLX tokenizer
    
    // MLXLMCommon types would go here when available

    init(modelDirectory: String) throws {
        self.modelPath = modelDirectory
        
        // Check if it's a directory or a single file
        var isDirectory: ObjCBool = false
        guard FileManager.default.fileExists(atPath: modelDirectory, isDirectory: &isDirectory) else {
            throw LLMError.modelNotFound
        }
        
        if !isDirectory.boolValue {
            // For single files (like .safetensors), we need the parent directory
            let url = URL(fileURLWithPath: modelDirectory)
            let directory = url.deletingLastPathComponent().path
            self.modelPath = directory
            print("MLX: Using model directory: \(directory)")
        }
    }
    
    #if canImport(MLXLMCommon)
    func loadModel() async throws {
        NSLog("üö® OLD MLXModelWrapper.loadModel() called with path: %@", modelPath)
        // Load the model using MLX-LM
        do {
            let modelURL = URL(fileURLWithPath: modelPath)
            
            // Try to load using the model directory structure
            // MLX models typically have config.json, tokenizer.json, and weights
            let configURL = modelURL.appendingPathComponent("config.json")
            let tokenizerURL = modelURL.appendingPathComponent("tokenizer.json")
            
            print("üö® OLD wrapper checking for config.json at: \(configURL.path)")
            
            // Check if files exist
            guard FileManager.default.fileExists(atPath: configURL.path) else {
                print("üö® OLD wrapper config.json NOT FOUND")
                throw LLMError.custom("config.json not found in model directory")
            }
            
            // For now, we'll store the paths - actual MLX-LM loading would happen here
            // The real implementation would use MLX-LM's model loading API
            print("‚úÖ Found MLX model files:")
            print("  - Config: \(configURL.path)")
            print("  - Tokenizer: \(tokenizerURL.path)")
            
            // Note: Full MLX-LM integration would require:
            // 1. Model architecture implementation (Llama, Mistral, etc.)
            // 2. Weight loading from safetensors/npz files
            // 3. Proper tokenizer loading
            
            throw LLMError.custom("""
                MLX-LM model loading requires full implementation of:
                1. Model architecture (Llama, Mistral, Gemma, etc.)
                2. Weight loading from safetensors format
                3. Tokenizer integration
                
                The MLXLMCommon package provides the foundation, but each model
                architecture needs to be implemented separately.
                
                Consider using the llama.cpp framework for immediate functionality.
                """)
            
        } catch {
            print("‚ùå Failed to load MLX-LM model: \(error)")
            throw error
        }
    }
    #endif
}

private class MLXTokenizer {
    private let tokenizerPath: String
    private var vocab: [String: Int] = [:]
    private var inverseVocab: [Int: String] = [:]

    init(tokenizerPath: String) throws {
        self.tokenizerPath = tokenizerPath
        try loadTokenizer()
    }

    private func loadTokenizer() throws {
        // If we have a tokenizer path, try to load it
        if !tokenizerPath.isEmpty && FileManager.default.fileExists(atPath: tokenizerPath) {
            // In real implementation, this would load from tokenizer.json
            // For now, we'll use the basic vocabulary
        }
        
        // For demonstration, create a vocabulary based on common MLX models
        let commonTokens = [
            "<s>": 1, "</s>": 2, "<unk>": 3, "<pad>": 0,
            "‚ñÅthe": 4, "‚ñÅand": 5, "‚ñÅto": 6, "‚ñÅof": 7, "‚ñÅa": 8, "‚ñÅin": 9, "‚ñÅis": 10,
            "‚ñÅthat": 11, "‚ñÅit": 12, "‚ñÅyou": 13, "‚ñÅfor": 14, "‚ñÅon": 15, "‚ñÅwith": 16,
            "‚ñÅas": 17, "‚ñÅare": 18, "‚ñÅwas": 19, "‚ñÅat": 20, "‚ñÅbe": 21, "‚ñÅor": 22,
            "‚ñÅan": 23, "‚ñÅbut": 24, "‚ñÅnot": 25, "‚ñÅthis": 26, "‚ñÅhave": 27, "‚ñÅfrom": 28,
            "‚ñÅthey": 29, "‚ñÅshe": 30, "‚ñÅhe": 31, "‚ñÅwe": 32, "‚ñÅI": 33, "‚ñÅcan": 34
        ]

        for (token, id) in commonTokens {
            vocab[token] = id
            inverseVocab[id] = token
        }
    }

    func encode(_ text: String) -> [Int32] {
        // Real MLX tokenizer would use proper subword tokenization
        let words = text.components(separatedBy: .whitespacesAndNewlines)
        var tokens: [Int32] = [1] // Start token

        for word in words {
            let tokenKey = "‚ñÅ" + word.lowercased()
            if let tokenId = vocab[tokenKey] {
                tokens.append(Int32(tokenId))
            } else {
                // Use hash for unknown tokens
                tokens.append(Int32(abs(word.hashValue) % 5000 + 100))
            }
        }

        return tokens
    }

    func decode(_ tokens: [Int32]) -> String {
        // Decode tokens using vocabulary
        return tokens.compactMap { tokenId in
            // Look up token in vocabulary
            if let word = inverseVocab[Int(tokenId)] {
                return word.replacingOccurrences(of: "‚ñÅ", with: " ")
            }
            // Unknown token
            return "<unk>"
        }.joined(separator: "")
    }
}

@available(iOS 17.0, *)
class MLXService: BaseLLMService {
    // MARK: - MLX Specific Capabilities

    override var supportsStreaming: Bool { true }
    override var supportsQuantization: Bool { true }
    override var supportsBatching: Bool { true }
    override var supportsMultiModal: Bool { false }
    override var quantizationFormats: [QuantizationFormat] { [.int4, .int8, .fp16] }
    override var maxContextLength: Int { 131072 } // Depends on model
    override var supportsCustomOperators: Bool { true }
    override var hardwareAcceleration: [HardwareAcceleration] { [.gpu, .cpu, .metal, .mps] }
    override var supportedFormats: [ModelFormat] { [.mlx, .safetensors] }

    override var frameworkInfo: FrameworkInfo {
        FrameworkInfo(
            name: "MLX",
            version: "0.6.0",
            developer: "Apple Inc.",
            description: "Apple's array framework for machine learning on Apple Silicon with unified memory",
            website: URL(string: "https://ml-explore.github.io/mlx/"),
            documentation: URL(string: "https://ml-explore.github.io/mlx/build/html/index.html"),
            minimumOSVersion: "17.0",
            requiredCapabilities: ["apple-silicon"],
            optimizedFor: [.appleNeuralEngine, .metalPerformanceShaders, .memoryEfficient, .highThroughput],
            features: [
                .onDeviceInference,
                .customModels,
                .quantization,
                .batching,
                .customOperators,
                .openSource,
                .offlineCapable
            ]
        )
    }

    override var name: String { "MLX" }

    override var supportedModels: [ModelInfo] {
        get {
            [
                ModelInfo(
                    id: "mistral-7b-mlx-4bit",
                    name: "Mistral-7B-Instruct-v0.2-4bit",
                    format: .mlx,
                    size: "3.8GB",
                    framework: .mlx,
                    quantization: "4-bit",
                    contextLength: 32768,
                    downloadURL: URL(string: "https://huggingface.co/mlx-community/Mistral-7B-Instruct-v0.2-4bit/resolve/main/Mistral-7B-Instruct-v0.2-4bit.tar.gz")!,
                    description: "Mistral 7B optimized for Apple Silicon with MLX 4-bit quantization",
                    minimumMemory: 6_000_000_000,
                    recommendedMemory: 8_000_000_000
                ),
                ModelInfo(
                    id: "llama-3.2-3b-mlx",
                    name: "Llama-3.2-3B-Instruct-4bit",
                    format: .mlx,
                    size: "1.7GB",
                    framework: .mlx,
                    quantization: "4-bit",
                    contextLength: 131072,
                    downloadURL: URL(string: "https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct-4bit/resolve/main/Llama-3.2-3B-Instruct-4bit.tar.gz")!,
                    description: "Llama 3.2 3B model with MLX acceleration for Apple Silicon",
                    minimumMemory: 3_000_000_000,
                    recommendedMemory: 4_000_000_000
                ),
                ModelInfo(
                    id: "gemma-2b-mlx",
                    name: "gemma-2b-it-4bit",
                    format: .mlx,
                    size: "1.2GB",
                    framework: .mlx,
                    quantization: "4-bit",
                    contextLength: 8192,
                    downloadURL: URL(string: "https://huggingface.co/mlx-community/gemma-2b-it-4bit/resolve/main/gemma-2b-it-4bit.tar.gz")!,
                    description: "Google's Gemma 2B instruction-tuned model optimized for MLX",
                    minimumMemory: 2_000_000_000,
                    recommendedMemory: 3_000_000_000
                )
            ]
        }
        set {}
    }

    private var currentModelInfo: ModelInfo?
    private var mlxModel: MLXModelWrapper?
    private var tokenizer: MLXTokenizer?
    private var realTokenizer: Tokenizer?  // Real tokenizer from TokenizerFactory
    private var modelImplementation: MLXModelImplementation?

    override func initialize(modelPath: String) async throws {
        NSLog("üî• MLX initialize called with modelPath: '%@'", modelPath)
        // Check if device supports MLX (A17 Pro/M3 or newer)
        guard isMLXSupported() else {
            throw LLMError.frameworkNotSupported
        }

        // First, try to identify the model from the path
        await MainActor.run {
            currentModelInfo = supportedModels.first { modelInfo in
                modelPath.contains(modelInfo.name) || modelPath.contains(modelInfo.id)
            }
        }

        // Debug: Always check what we're starting with
        print("MLX: Initial modelPath = '\(modelPath)'")
        print("MLX: Current model info = \(currentModelInfo?.name ?? "none")")
        
        // Always check if config.json exists in the main Models directory first
        let modelsDir = ModelManager.modelsDirectory
        let mainConfigPath = modelsDir.appendingPathComponent("config.json")
        
        var actualModelPath = modelPath
        
        print("MLX: Checking for config.json at: \(mainConfigPath.path)")
        if FileManager.default.fileExists(atPath: mainConfigPath.path) {
            print("MLX: ‚úÖ Found config.json in main Models directory")
            // Use the Models directory as the model path
            actualModelPath = modelsDir.path
        } else {
            print("MLX: config.json not found in main Models directory")
            
            // List contents of Models directory for debugging
            do {
                let contents = try FileManager.default.contentsOfDirectory(atPath: modelsDir.path)
                print("MLX: Models directory contents: \(contents.joined(separator: ", "))")
            } catch {
                print("MLX: Could not read Models directory: \(error)")
            }
            
            // If modelPath is empty, try to find the model in the downloaded models
            if modelPath.isEmpty || !FileManager.default.fileExists(atPath: modelPath) {
                print("MLX: Model path empty or not found, searching for model...")
                
                // Look for the model in the Models directory
                let modelManager = await MainActor.run { ModelManager.shared }
                
                // Try to find based on the current selection or path hints
                var searchNames: [String] = []
                
                // Add names from currentModelInfo if available
                if let modelInfo = currentModelInfo {
                    searchNames.append(modelInfo.name)
                    searchNames.append(modelInfo.name.replacingOccurrences(of: "-4bit", with: ""))
                    searchNames.append(modelInfo.name.replacingOccurrences(of: "4bit", with: ""))
                }
                
                // Add names from the path
                let pathComponents = modelPath.components(separatedBy: "/")
                if let fileName = pathComponents.last {
                    searchNames.append(fileName)
                    searchNames.append(fileName.replacingOccurrences(of: "-4bit", with: ""))
                    searchNames.append(fileName.replacingOccurrences(of: "4bit", with: ""))
                }
                
                print("MLX: Search names: \(searchNames)")
                
                // Search for the model
                var foundPath: String? = nil
                
                for searchName in searchNames {
                    // First try the framework-specific path
                    let possiblePath = await modelManager.modelPath(for: searchName, framework: .mlx)
                    print("MLX: Checking framework path: \(possiblePath.path)")
                    if FileManager.default.fileExists(atPath: possiblePath.path) {
                        var isDir: ObjCBool = false
                        if FileManager.default.fileExists(atPath: possiblePath.path, isDirectory: &isDir), isDir.boolValue {
                            foundPath = possiblePath.path
                            print("MLX: Found model directory at \(foundPath!)")
                            break
                        }
                    }
                }
                
                if let foundPath = foundPath {
                    actualModelPath = foundPath
                } else {
                    print("MLX: Could not find model directory. Searched names: \(searchNames)")
                    throw LLMError.modelNotFound
                }
            }
        }

        // Check if the path is a directory or file
        var isDirectory: ObjCBool = false
        FileManager.default.fileExists(atPath: actualModelPath, isDirectory: &isDirectory)
        
        // If actualModelPath points to a file, use its parent directory
        if !isDirectory.boolValue {
            let url = URL(fileURLWithPath: actualModelPath)
            actualModelPath = url.deletingLastPathComponent().path
            print("MLX: Model path was a file, using parent directory: \(actualModelPath)")
        } else {
            print("MLX: Model path is directory: \(actualModelPath)")
        }

        // Update currentModelInfo if not already set
        if currentModelInfo == nil {
            let pathToCheck = actualModelPath
            await MainActor.run {
                currentModelInfo = supportedModels.first { modelInfo in
                    pathToCheck.contains(modelInfo.name) || pathToCheck.contains(modelInfo.id)
                }
            }
        }

        // Real MLX initialization:
        #if canImport(MLX)
        // Actual MLX initialization code would go here
        // For now, we simulate the process as full LLM loading is complex
        print("MLX Framework available - using real MLX backend")

        // Initialize MLX with GPU support
        // In real implementation:
        // let device = Device.gpu
        // let model = try await LLM.load(modelPath, device: device)

        // Simulate model loading with proper delay for MLX models
        try await Task.sleep(nanoseconds: 2_000_000_000) // 2 seconds with MLX acceleration
        #else
        throw LLMError.frameworkNotSupported
        #endif

        // Initialize wrapper
        mlxModel = try MLXModelWrapper(modelDirectory: actualModelPath)
        
        // Initialize the new model implementation
        NSLog("MLX: Creating MLXModelImplementation with path: %@", actualModelPath)
        modelImplementation = MLXModelImplementation(modelPath: actualModelPath)
        
        do {
            // Try to load the model with the new implementation
            NSLog("MLX: About to call modelImplementation.loadModel()")
            try await modelImplementation?.loadModel()
            print("‚úÖ Successfully loaded MLX model with new implementation")
            
            // Keep the basic tokenizer for now
            tokenizer = try MLXTokenizer(tokenizerPath: "")
        } catch {
            print("‚ö†Ô∏è New implementation load failed, falling back to MLXLMCommon: \(error)")
            
            #if canImport(MLXLMCommon)
            // Load the actual MLX-LM model
            if let wrapper = mlxModel {
                try await wrapper.loadModel()
                
                // Also keep the basic tokenizer for compatibility
                tokenizer = try MLXTokenizer(tokenizerPath: "")
            }
            #else
            // Fallback to basic tokenizer
            if let model = mlxModel {
                tokenizer = try MLXTokenizer(tokenizerPath: "")
            }
            #endif
        }

        print("MLX Model initialized successfully:")
        print("- Path: \(actualModelPath)")
        print("- Device support: Apple Silicon optimized")
        print("- Memory: GPU acceleration enabled")

        isInitialized = true
    }

    override func generate(prompt: String, options: GenerationOptions) async throws -> String {
        guard isInitialized, mlxModel != nil, tokenizer != nil else {
            throw LLMError.notInitialized()
        }

        var result = ""
        try await streamGenerate(prompt: prompt, options: options) { token in
            result += token
        }

        return result
    }

    override func streamGenerate(
        prompt: String,
        options: GenerationOptions,
        onToken: @escaping (String) -> Void
    ) async throws {
        guard isInitialized, let _ = mlxModel, let _ = tokenizer else {
            let error = LLMError.notInitialized()
            print("‚ùå MLX Service not initialized: \(error)")
            throw error
        }

        // Try using the new model implementation first
        if let implementation = modelImplementation {
            print("üöÄ Using new MLX model implementation")
            
            do {
                let stream = try await implementation.generate(
                    prompt: prompt,
                    maxTokens: options.maxTokens,
                    temperature: options.temperature
                )
                
                for try await token in stream {
                    await MainActor.run {
                        onToken(token)
                    }
                }
                
                return
            } catch {
                print("‚ö†Ô∏è New implementation failed: \(error)")
                // Fall through to old error message
            }
        }

        #if canImport(MLX)
        // MLX framework is available
        print("üîç Checking MLX-LM integration status...")
        
        let error = LLMError.custom("""
            MLX framework is detected but full LLM support requires additional implementation.
            
            Current status:
            ‚úì MLX framework: Available
            ‚úì Model path: \(mlxModel?.modelPath ?? "unknown")
            ‚úì MLXLMCommon: Available
            ‚úó Model implementation: Not complete
            
            MLX-LM requires model-specific implementations for:
            ‚Ä¢ Llama models: LlamaModel class
            ‚Ä¢ Mistral models: MistralModel class  
            ‚Ä¢ Gemma models: GemmaModel class
            ‚Ä¢ Phi models: PhiModel class
            
            Each model architecture needs:
            1. Attention mechanism implementation
            2. Feed-forward network implementation
            3. Embedding and output layers
            4. Safetensors weight loading
            
            This is a significant implementation effort. For immediate use:
            ‚Ä¢ Use llama.cpp for GGUF models (fully implemented)
            ‚Ä¢ Use Core ML for .mlpackage models (fully implemented)
            
            Device compatibility: \(isMLXSupported() ? "‚úì Compatible with MLX" : "‚ö†Ô∏è Limited MLX support")
            """)
        
        print("‚ùå MLX-LM model implementation not complete: \(error)")
        throw error
        #else
        // MLX framework is not available in the current build
        let error = LLMError.custom("""
            MLX framework is not available or properly configured.
            
            To use MLX models:
            1. Ensure you're running on Apple Silicon (A17 Pro/M3 or newer)
            2. The MLX Swift package must be properly integrated
            3. The model must be in MLX format (.safetensors)
            
            Current status:
            - Model loaded: ‚úì
            - Framework available: ‚úó
            
            Please try using a different framework like llama.cpp or ONNX Runtime.
            """)
        
        print("‚ùå MLX framework not available: \(error)")
        throw error
        #endif
    }

    // MARK: - MLX Implementation Helper Methods

    #if canImport(MLX)
    private func createMLXArray(from tokens: [Int32]) throws -> MLX.MLXArray {
        // Create MLX array from tokens for GPU computation
        // MLX arrays are the fundamental data structure for Apple Silicon computation
        let shape = [1, tokens.count] // Batch size 1, sequence length

        // Convert tokens to MLX array
        let data = tokens.map { Int($0) }
        return MLX.MLXArray(data).reshaped(shape)
    }

    private func performMLXForward(input: MLX.MLXArray, model: MLXModelWrapper) async throws -> MLX.MLXArray {
        // Real MLX model forward pass not implemented
        throw LLMError.custom("""
            MLX model inference is not fully implemented.
            
            Required implementation:
            1. Load actual MLX model weights from safetensors file
            2. Implement transformer forward pass
            3. Return proper logits tensor
            
            This requires the full MLX-LM package integration.
            """)
    }

    private func sampleMLXToken(from logits: MLX.MLXArray, temperature: Float, step: Int) throws -> Int32 {
        // Real MLX implementation required
        throw LLMError.notImplemented
    }
    #endif

    // MARK: - Private MLX Helper Methods

    override func getModelInfo() -> ModelInfo? {
        currentModelInfo
    }

    override func cleanup() {
        // In real MLX implementation:
        // model?.cleanup()
        // MLX.clearCache()

        mlxModel = nil
        tokenizer = nil
        modelImplementation = nil
        currentModelInfo = nil
        isInitialized = false

        // Force memory cleanup for GPU resources
        Task {
            await Task.yield()
        }
    }

    private func isMLXSupported() -> Bool {
        // Check for Apple Silicon with sufficient GPU cores
        // A17 Pro (iPhone 15 Pro), M3, or newer

        var systemInfo = utsname()
        uname(&systemInfo)
        let modelName = String(bytes: Data(bytes: &systemInfo.machine, count: Int(_SYS_NAMELEN)), encoding: .ascii)?.trimmingCharacters(in: .controlCharacters)

        guard let model = modelName else { return false }

        // Check for compatible devices
        let supportedModels = [
            "iPhone16,1",  // iPhone 15 Pro
            "iPhone16,2",  // iPhone 15 Pro Max
            "iPhone17,1",  // iPhone 16 Pro
            "iPhone17,2",  // iPhone 16 Pro Max
            "arm64"        // M-series Macs and simulators
        ]

        let isSupported = supportedModels.contains { model.contains($0) }

        if !isSupported {
            print("MLX Warning: Device \(model) may have limited MLX support")
            print("MLX works best on A17 Pro, M3, or newer Apple Silicon")
        }

        return true // Allow for demo purposes, but log warning
    }

    deinit {
        cleanup()
    }
}
