/**
 * @file llamacpp_generator.cpp
 * @brief LlamaCPP text generator implementation
 */

#include "llamacpp_generator.h"
#include "rac/core/rac_logger.h"
#include "../llamacpp/llamacpp_backend.h"

#include <nlohmann/json.hpp>

#define LOG_TAG "RAG.LlamaCppGen"
#define LOGI(...) RAC_LOG_INFO(LOG_TAG, __VA_ARGS__)
#define LOGE(...) RAC_LOG_ERROR(LOG_TAG, __VA_ARGS__)

namespace runanywhere {
namespace rag {

// =============================================================================
// PIMPL IMPLEMENTATION
// =============================================================================

class LlamaCppGenerator::Impl {
public:
    explicit Impl(const std::string& model_path, const std::string& config_json)
        : model_path_(model_path) {
        
        // Parse config
        nlohmann::json config;
        if (!config_json.empty()) {
            try {
                config = nlohmann::json::parse(config_json);
            } catch (const std::exception& e) {
                LOGE("Failed to parse config JSON: %s", e.what());
                config = nlohmann::json::object();
            }
        } else {
            config = nlohmann::json::object();
        }

        // Add model path to config
        config["model_path"] = model_path;
        
        // Set default parameters for RAG usage
        if (!config.contains("n_ctx")) {
            config["n_ctx"] = 4096;  // Context size
        }
        if (!config.contains("n_gpu_layers")) {
            config["n_gpu_layers"] = 0;  // CPU-only for compatibility
        }

        // Create LlamaCPP backend
        backend_ = std::make_unique<LlamaCppBackend>();
        
        // Initialize backend
        if (!backend_->initialize(config)) {
            throw std::runtime_error("Failed to initialize LlamaCPP backend");
        }

        // Load model
        if (!backend_->load_model(model_path, config)) {
            throw std::runtime_error("Failed to load model: " + model_path);
        }

        ready_ = true;
        context_size_ = config.value("n_ctx", 4096);
        
        LOGI("LlamaCPP generator initialized: %s", model_path.c_str());
    }

    ~Impl() {
        if (backend_) {
            backend_->cleanup();
        }
    }

    GenerationResult generate(
        const std::string& prompt,
        const GenerationOptions& options
    ) {
        if (!ready_) {
            throw std::runtime_error("LlamaCPP generator not ready");
        }

        // Convert options to LlamaCPP format
        TextGenerationOptions llama_opts;
        llama_opts.max_tokens = options.max_tokens;
        llama_opts.temperature = options.temperature;
        llama_opts.top_p = options.top_p;
        llama_opts.top_k = options.top_k;
        llama_opts.use_sampling = options.use_sampling;
        llama_opts.stop_sequences = options.stop_sequences;

        // Generate text
        TextGenerationResult llama_result;
        if (!backend_->generate(prompt, llama_opts, &llama_result)) {
            throw std::runtime_error("Text generation failed");
        }

        // Convert result
        GenerationResult result;
        result.text = std::move(llama_result.text);
        result.tokens_generated = llama_result.tokens_generated;
        result.prompt_tokens = llama_result.prompt_tokens;
        result.inference_time_ms = llama_result.inference_time_ms;
        result.finished = (llama_result.finish_reason == "stop");
        result.stop_reason = std::move(llama_result.finish_reason);

        return result;
    }

    bool is_ready() const noexcept {
        return ready_;
    }

    int context_size() const noexcept {
        return context_size_;
    }

private:
    std::string model_path_;
    std::unique_ptr<LlamaCppBackend> backend_;
    bool ready_ = false;
    int context_size_ = 4096;
};

// =============================================================================
// PUBLIC API
// =============================================================================

LlamaCppGenerator::LlamaCppGenerator(
    const std::string& model_path,
    const std::string& config_json
) : impl_(std::make_unique<Impl>(model_path, config_json)) {
}

LlamaCppGenerator::~LlamaCppGenerator() = default;

LlamaCppGenerator::LlamaCppGenerator(LlamaCppGenerator&&) noexcept = default;
LlamaCppGenerator& LlamaCppGenerator::operator=(LlamaCppGenerator&&) noexcept = default;

GenerationResult LlamaCppGenerator::generate(
    const std::string& prompt,
    const GenerationOptions& options
) {
    return impl_->generate(prompt, options);
}

bool LlamaCppGenerator::is_ready() const noexcept {
    return impl_->is_ready();
}

const char* LlamaCppGenerator::name() const noexcept {
    return "LlamaCPP-Generator";
}

int LlamaCppGenerator::context_size() const noexcept {
    return impl_->context_size();
}

// =============================================================================
// FACTORY FUNCTION
// =============================================================================

std::unique_ptr<ITextGenerator> create_llamacpp_generator(
    const std::string& model_path,
    const std::string& config_json
) {
    return std::make_unique<LlamaCppGenerator>(model_path, config_json);
}

} // namespace rag
} // namespace runanywhere
